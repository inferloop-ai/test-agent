# LLM Provider Configuration
# Options: ollama, openai, anthropic, azure_openai
LLM_PROVIDER=ollama
LLM_MODEL=qwen2.5:7b

# Prefer local models over cloud APIs (true/false)
PREFER_LOCAL_LLM=true

# Ollama Configuration (for local models)
OLLAMA_BASE_URL=http://localhost:11434

# OpenAI Configuration (optional)
# Get your API key from https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Anthropic Configuration (optional)
# Get your API key from https://console.anthropic.com/
ANTHROPIC_API_KEY=

# Azure OpenAI Configuration (optional)
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_DEPLOYMENT=
AZURE_OPENAI_API_VERSION=2023-12-01-preview

# Output Configuration
OUTPUT_DIR=./outputs